{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a **statistical model** used primarily for **binary classification tasks**. It predicts the probability that a given input belongs to a particular class, making it particularly useful when the output is categorical, like \"yes or no,\" \"success or failure,\" or \"spam or not spam.\" Despite its name, logistic regression is a **classification algorithm**, not a regression algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts of Logistic Regression**\n",
    "\n",
    "1. **Logistic Function (Sigmoid Function):**\n",
    "   - Logistic regression uses the sigmoid function to map predicted values (from \\(-\\infty\\) to \\(+\\infty\\)) to probabilities (between 0 and 1).\n",
    "   - The sigmoid function is defined as:\n",
    "     \\[\n",
    "     \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "     \\]\n",
    "     where \\(z = w^T x + b\\), \\(w\\) is the weights vector, \\(x\\) is the input feature vector, and \\(b\\) is the bias term.\n",
    "\n",
    "2. **Output Interpretation:**\n",
    "   - The output of the sigmoid function is a probability:\n",
    "     \\[\n",
    "     P(y=1|x) = \\sigma(z)\n",
    "     \\]\n",
    "   - If the probability is greater than a threshold (commonly 0.5), the model predicts the class as \\(y=1\\); otherwise, \\(y=0\\).\n",
    "\n",
    "3. **Cost Function:**\n",
    "   - Logistic regression uses the **log-loss (binary cross-entropy)** as its cost function:\n",
    "     \\[\n",
    "     J(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]\n",
    "     \\]\n",
    "     where:\n",
    "     - \\(m\\): Number of training examples\n",
    "     - \\(y^{(i)}\\): Actual label (0 or 1)\n",
    "     - \\(\\hat{y}^{(i)}\\): Predicted probability\n",
    "\n",
    "4. **Training Process:**\n",
    "   - The model learns parameters (\\(w\\) and \\(b\\)) using optimization techniques like **Gradient Descent** to minimize the cost function.\n",
    "\n",
    "5. **Multiclass Classification (Extension):**\n",
    "   - For multiclass classification, logistic regression can be extended using:\n",
    "     - **One-vs-Rest (OvR):** Separate binary classifiers for each class.\n",
    "     - **Softmax Regression:** Generalization of logistic regression to handle multiple classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages:**\n",
    "- Simple and easy to implement.\n",
    "- Computationally efficient.\n",
    "- Provides probabilities, which help in understanding model confidence.\n",
    "- Performs well when the relationship between features and the target variable is approximately linear.\n",
    "\n",
    "### **Limitations:**\n",
    "- Assumes a linear relationship between the independent variables and the log-odds.\n",
    "- Not suitable for complex relationships unless combined with feature engineering.\n",
    "- Sensitive to outliers and irrelevant features.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications:**\n",
    "- Email spam detection.\n",
    "- Medical diagnosis (e.g., disease presence).\n",
    "- Customer churn prediction.\n",
    "- Fraud detection in banking.\n",
    "\n",
    "Logistic regression is widely used as a baseline model due to its simplicity and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "# Generate the dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,       # Number of samples\n",
    "    n_features=10,        # Total number of features\n",
    "    n_informative=5,      # Number of informative features\n",
    "    n_redundant=2,        # Number of redundant features\n",
    "    n_classes=2,          # Number of output classes (binary)\n",
    "    flip_y=0.01,          # Percentage of labels to flip (noise)\n",
    "    class_sep=1.5,        # Separation between the classes\n",
    "    random_state=42       # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "data = pd.DataFrame(X, columns=[f\"Feature_{i}\" for i in range(1, 11)])\n",
    "data['Target'] = y\n",
    "\n",
    "# Display the first few rows\n",
    "print(data.head())\n",
    "\n",
    "# Save to CSV (optional)\n",
    "data.to_csv(\"binary_classification_dataset.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
